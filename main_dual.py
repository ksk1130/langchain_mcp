# Create server parameters for stdio connection

import os
import gradio as gr
import asyncio
import json
from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

global_client = None
global_tools = []


async def main() -> None:
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”ã‹ã‚‰å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    def extract_answer(resp) -> str:
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”ã‹ã‚‰å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ã€‚
        dict, list, ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‹ã«å¯¾å¿œã—ã€AIMessageã®contentã‚„ä»£è¡¨çš„ãªå›ç­”ã‚­ãƒ¼ã‚’å„ªå…ˆã—ã¦è¿”ã™ã€‚
        Args:
            resp: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰è¿”ã•ã‚ŒãŸå¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        Returns:
            str: å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if isinstance(resp, dict):
            if "messages" in resp:
                messages = resp["messages"]
                if isinstance(messages, list):
                    for msg in reversed(messages):
                        if (
                            hasattr(msg, "content")
                            and hasattr(msg, "__class__")
                            and "AIMessage" in str(type(msg))
                        ):
                            return msg.content
                        elif isinstance(msg, dict) and msg.get("type") == "ai":
                            return msg.get("content", "")
            for key in ["output", "answer", "content", "result", "text"]:
                if key in resp:
                    return extract_answer(resp[key])
            if resp.get("type") == "ai":
                return resp.get("content", "")
            return str(resp)
        elif isinstance(resp, list):
            for item in reversed(resp):
                if (
                    hasattr(item, "content")
                    and hasattr(item, "__class__")
                    and "AIMessage" in str(type(item))
                ):
                    return item.content
                elif isinstance(item, dict) and item.get("type") == "ai":
                    return item.get("content", "")
            answers = [extract_answer(item) for item in resp]
            return "\n---\n".join(str(a) for a in answers if a)
        else:
            if hasattr(resp, "content"):
                return resp.content
            try:
                return str(resp)
            except Exception:
                return f"<{type(resp).__name__}>"

    # server_params.jsonã‹ã‚‰ã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚’èª­ã¿è¾¼ã¿
    with open("server_params.json", encoding="utf-8") as f:
        params = json.load(f)

    # åˆ©ç”¨å¯èƒ½ãªLLMã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    llm_options = params.get("llm", {})
    available_llms = list(llm_options.keys()) if llm_options else ["Default"]

    # 2ã¤ã®LLMã‚’å–å¾—ï¼ˆæœ€åˆã®2ã¤ã€ã¾ãŸã¯åŒã˜ã‚‚ã®ã‚’2å›ï¼‰
    llm1_name = available_llms[0] if len(available_llms) >= 1 else "Default"
    llm2_name = available_llms[1] if len(available_llms) >= 2 else available_llms[0]

    # LLMã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
    def initialize_llm(llm_name: str) -> ChatOpenAI:
        """é¸æŠã•ã‚ŒãŸLLMã«åŸºã¥ã„ã¦ChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–"""
        if llm_name in llm_options:
            llm_config = llm_options[llm_name]
            if isinstance(llm_config, dict):
                # æ–°ã—ã„å½¢å¼: {"model": "...", "base_url": "..."}
                model = llm_config.get("model", "gpt-4o")
                base_url = llm_config.get("base_url")
                if base_url:
                    return ChatOpenAI(model=model, base_url=base_url)
                else:
                    return ChatOpenAI(model=model)
            else:
                # å¤ã„å½¢å¼: æ–‡å­—åˆ—ã®base_url
                return ChatOpenAI(model="gpt-4o", base_url=llm_config)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯ä¸æ˜ãªLLMã®å ´åˆ
            base_url = params.get("base_url")
            if base_url:
                return ChatOpenAI(model="gpt-4o", base_url=base_url)
            else:
                return ChatOpenAI(model="gpt-4o")

    # ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«clientã¨toolsã‚’ä¸€åº¦å–å¾—ã—ã¦ä½¿ã„å›ã™
    print("=== MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ãƒ„ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–ä¸­... ===")

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ãƒ„ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
    try:
        global global_client
        global_client = MultiServerMCPClient(params.get("servers", {}))
        global global_tools
        global_tools = await global_client.get_tools()
        print(f"åˆæœŸåŒ–å®Œäº†: {len(global_tools)} å€‹ã®ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™")

        # ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        for i, tool in enumerate(global_tools, 1):
            tool_name = getattr(tool, "name", "Unknown")
            print(f"  {i}. {tool_name}")
    except Exception as e:
        print(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        global_client = None
        global_tools = []

    # å˜ä¸€LLMç”¨ã®éåŒæœŸãƒãƒ£ãƒƒãƒˆé–¢æ•°
    async def single_llm_chat(user_input, history, function_calling, llm_name) -> str:
        """
        å˜ä¸€ã®LLMã«å¯¾ã—ã¦ãƒãƒ£ãƒƒãƒˆã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
        """
        try:
            # é¸æŠã•ã‚ŒãŸLLMã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            current_llm = initialize_llm(llm_name)

            # Gradioã®å±¥æ­´(messageså½¢å¼)ã‚’LangChainã®å±¥æ­´ã«å¤‰æ›
            messages = []
            if history:
                for msg in history:
                    if msg.get("role") == "user":
                        messages.append({"type": "human", "content": msg["content"]})
                    elif msg.get("role") == "assistant":
                        messages.append({"type": "ai", "content": msg["content"]})
            messages.append({"type": "human", "content": user_input})

            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
            agent_tools = global_tools if function_calling == "æœ‰åŠ¹" else []

            agent = create_react_agent(current_llm, agent_tools, debug=True)
            agent_response = await agent.ainvoke({"messages": messages})
            answer = extract_answer(agent_response)

            # ãƒ„ãƒ¼ãƒ«å±¥æ­´æŠ½å‡º
            tool_history = []
            if isinstance(agent_response, dict):
                if "messages" in agent_response:
                    messages_resp = agent_response["messages"]
                    if isinstance(messages_resp, list):
                        for msg in messages_resp:
                            if hasattr(msg, "tool_calls") and msg.tool_calls:
                                for tool_call in msg.tool_calls:
                                    tool_name = (
                                        tool_call.get("name")
                                        if isinstance(tool_call, dict)
                                        else getattr(tool_call, "name", str(tool_call))
                                    )
                                    tool_args = (
                                        tool_call.get("args")
                                        if isinstance(tool_call, dict)
                                        else getattr(tool_call, "args", {})
                                    )
                                    tool_history.append(
                                        f"ãƒ„ãƒ¼ãƒ«å: {tool_name}, å¼•æ•°: {tool_args}"
                                    )
            if tool_history:
                answer += f"\n\n[{llm_name} - å‘¼ã³å‡ºã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«å±¥æ­´]\n" + "\n".join(
                    tool_history
                )

            return answer
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({llm_name}): {str(e)}"

    # ä¸¡æ–¹ã®LLMã«åŒæ™‚ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã™ã‚‹é–¢æ•°
    async def dual_llm_chat(user_input, history1, history2, function_calling) -> tuple:
        """
        2ã¤ã®LLMã«åŒæ™‚ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã€çµæœã‚’è¿”ã™
        """
        if not user_input.strip():
            return "", history1, "", history2

        # ä¸¡æ–¹ã®LLMã«åŒæ™‚ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        tasks = [
            single_llm_chat(user_input, history1, function_calling, llm1_name),
            single_llm_chat(user_input, history2, function_calling, llm2_name),
        ]

        responses = await asyncio.gather(*tasks, return_exceptions=True)

        response1 = (
            responses[0]
            if not isinstance(responses[0], Exception)
            else f"ã‚¨ãƒ©ãƒ¼: {responses[0]}"
        )
        response2 = (
            responses[1]
            if not isinstance(responses[1], Exception)
            else f"ã‚¨ãƒ©ãƒ¼: {responses[1]}"
        )

        # å±¥æ­´ã‚’æ›´æ–°
        new_history1 = history1 + [
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": response1},
        ]
        new_history2 = history2 + [
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": response2},
        ]

        return "", new_history1, "", new_history2

    def sync_dual_llm_chat(user_input, history1, history2, function_calling) -> tuple:
        """
        éåŒæœŸdual_llm_chaté–¢æ•°ã‚’åŒæœŸçš„ã«å‘¼ã³å‡ºã™ãƒ©ãƒƒãƒ‘ãƒ¼
        """
        return asyncio.run(
            dual_llm_chat(user_input, history1, history2, function_calling)
        )

    # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    async def get_available_tools() -> str:
        """
        åˆæœŸåŒ–æ¸ˆã¿ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ„ãƒ¼ãƒ«ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—
        """
        try:
            result = "# åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§\n\n"
            result += f"**åˆè¨ˆãƒ„ãƒ¼ãƒ«æ•°**: {len(global_tools)}\n\n"

            if global_tools:
                result += "## ãƒ„ãƒ¼ãƒ«è©³ç´°:\n"
                for i, tool in enumerate(global_tools, 1):
                    tool_name = getattr(tool, "name", "Unknown")
                    tool_desc = getattr(tool, "description", "èª¬æ˜ãªã—")
                    tool_args = getattr(tool, "args_schema", {})

                    result += f"{i}. **{tool_name}**\n"
                    result += f"   - èª¬æ˜: {tool_desc}\n"
                    if tool_args:
                        result += f"   - å¼•æ•°: {tool_args}\n"
                    result += "\n"
            else:
                result += "åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"

            return result

        except Exception as e:
            return f"ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {str(e)}"

    def sync_get_available_tools() -> str:
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«å–å¾—ã®åŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼"""
        try:
            return asyncio.run(asyncio.wait_for(get_available_tools(), timeout=30.0))
        except asyncio.TimeoutError:
            return "ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        except Exception as e:
            return f"ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {str(e)}"

    # Gradio UIã®æ§‹ç¯‰
    with gr.Blocks(
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            height: 100vh !important;
            display: flex !important;
            flex-direction: column !important;
            padding: 0 !important;
            margin: 0 !important;
        }
        .title {
            flex-shrink: 0 !important;
            padding: 1px !important;
            margin: 0 !important;
        }
        .dual-chat-container {
            flex: 1 !important;
            display: flex !important;
            gap: 5px !important;
            padding: 1px !important;
        }
        .chat-pane {
            flex: 1 !important;
            display: flex !important;
            flex-direction: column !important;
        }
        .controls-container {
            flex-shrink: 0 !important;
            padding: 1px !important;
            background: #f8f9fa !important;
        }
        .input-container {
            flex-shrink: 0 !important;
            padding: 1px !important;
            background: white !important;
        }
        .chatbot {
            height: calc(100vh - 350px) !important;
            min-height: 400px !important;
        }
        .llm-title {
            text-align: center !important;
            font-weight: bold !important;
            color: #2563eb !important;
            padding: 5px !important;
        }
        """,
    ) as demo:
        gr.Markdown("# LangChain MCP ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒƒãƒˆ", elem_classes=["title"])

        # ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ†ã‘ã‚‹
        with gr.Tabs():
            with gr.TabItem("ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒƒãƒˆ"):
                # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«éƒ¨åˆ†
                with gr.Row(elem_classes=["controls-container"]):
                    gr.Markdown("**ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼ˆFunction Callingï¼‰:**")
                    function_radio = gr.Radio(
                        ["æœ‰åŠ¹", "ç„¡åŠ¹"], value="æœ‰åŠ¹", label=None, container=False
                    )

                # 2ã¤ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’æ¨ªä¸¦ã³ã§è¡¨ç¤º
                with gr.Row(elem_classes=["dual-chat-container"]):
                    # å·¦å´ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
                    with gr.Column(elem_classes=["chat-pane"]):
                        gr.Markdown(f"## {llm1_name}", elem_classes=["llm-title"])
                        chatbot1 = gr.Chatbot(
                            type="messages",
                            height="calc(100vh - 350px)",
                            container=True,
                            autoscroll=True,
                            show_copy_all_button=True,
                            show_copy_button=True,
                            resizable=True,
                            elem_classes=["chatbot"],
                        )

                    # å³å´ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
                    with gr.Column(elem_classes=["chat-pane"]):
                        gr.Markdown(f"## {llm2_name}", elem_classes=["llm-title"])
                        chatbot2 = gr.Chatbot(
                            type="messages",
                            height="calc(100vh - 350px)",
                            container=True,
                            autoscroll=True,
                            show_copy_all_button=True,
                            show_copy_button=True,
                            resizable=True,
                            elem_classes=["chatbot"],
                        )

                # å…±é€šã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
                with gr.Row(elem_classes=["input-container"]):
                    txt = gr.Textbox(
                        show_label=False,
                        placeholder="ä¸¡æ–¹ã®LLMã«åŒæ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¾ã™...",
                        container=False,
                        scale=9,
                        autofocus=True,
                        submit_btn=True,
                    )

            with gr.TabItem("åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«"):
                # ãƒ„ãƒ¼ãƒ«ä¸€è¦§è¡¨ç¤ºã‚¨ãƒªã‚¢
                tools_display = gr.Markdown(
                    "ã€Œãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’æ›´æ–°ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚",
                    height=600,
                )

                # ãƒ„ãƒ¼ãƒ«ä¸€è¦§æ›´æ–°ãƒœã‚¿ãƒ³
                refresh_tools_btn = gr.Button("ğŸ”„ ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’æ›´æ–°", variant="primary")

                def update_tools_display() -> str:
                    """ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’æ›´æ–°ã™ã‚‹é–¢æ•°"""
                    try:
                        tools_info = sync_get_available_tools()
                        return tools_info
                    except Exception as e:
                        return f"ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{str(e)}"

                refresh_tools_btn.click(update_tools_display, outputs=tools_display)

        def user_submit(user_input, history1, history2, function_calling) -> tuple:
            """
            ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ä¸¡æ–¹ã®LLMã«é€ä¿¡ã—ã€å±¥æ­´ã‚’æ›´æ–°ã™ã‚‹
            """
            result = sync_dual_llm_chat(
                user_input, history1, history2, function_calling
            )
            return result

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
        txt.submit(
            user_submit,
            [txt, chatbot1, chatbot2, function_radio],
            [txt, chatbot1, txt, chatbot2],
        )

    demo.launch(share=False, server_name="127.0.0.1", server_port=7860)


if __name__ == "__main__":
    asyncio.run(main())
